{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e979eb9-3fe0-49c2-b814-f4025ed81934",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook presents a RAG workflow for the [PubMed QA](https://pubmedqa.github.io/) task using [LlamaIndex](https://www.llamaindex.ai/). The code is written in a configurable fashion, giving you the flexibility to edit the RAG configuration and observe the change in output/responses.\n",
    "\n",
    "It covers a step-by-step procedure for building the RAG workflow (Stages 1-4) and later runs the pipeline on a sample from the dataset. The notebook also covers the sparse, dense, hybrid retrieval strategies along with the re-ranker. We have alse added an optional component for RAG evaluation using the [Ragas](https://docs.ragas.io/en/stable/getstarted/install.html) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e45a6-1ed6-4e90-b0b7-913566652b40",
   "metadata": {},
   "source": [
    "### Import libraries, custom classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82038178-2699-403b-b32a-342b8d19ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "from llama_index.core import ServiceContext, set_global_service_context, set_global_handler\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from task_dataset import PubMedQATaskDataset\n",
    "\n",
    "from utils.hosting_utils import RAGLLM\n",
    "from utils.rag_utils import (\n",
    "    DocumentReader, RAGEmbedding, RAGQueryEngine, RagasEval, \n",
    "    extract_yes_no, evaluate, validate_rag_cfg\n",
    "    )\n",
    "from utils.storage_utils import RAGIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdc2be2-3ab5-4cde-8f44-086a3547b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70d889-6e10-45b7-be3b-52a5b56d4f43",
   "metadata": {},
   "source": [
    "### Set RAG configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab816260-2e3f-4ac3-a3f7-623202b116bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg = {\n",
    "    # Node parser config\n",
    "    \"chunk_size\": 256,\n",
    "    \"chunk_overlap\": 0,\n",
    "\n",
    "    # Embedding model config\n",
    "    \"embed_model_type\": \"hf\",\n",
    "    \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
    "\n",
    "    # LLM config\n",
    "    \"llm_type\": \"local\",\n",
    "    \"llm_name\": \"Llama-2-7b-chat-hf\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 50,\n",
    "    \"do_sample\": False,\n",
    "\n",
    "    # Vector DB config\n",
    "    \"vector_db_type\": \"weaviate\", # \"weaviate\"\n",
    "    \"vector_db_name\": \"Pubmed_QA\",\n",
    "    \n",
    "    # MODIFY THIS\n",
    "    #\"weaviate_url\": \"https://rag-bootcamp-pubmed-qa-n3u138r8.weaviate.network\",\n",
    "    \"weaviate_url\": \"https://vector-rag-bootcamp-fqe3dp7f.weaviate.network\",\n",
    "\n",
    "    # Retriever and query config\n",
    "    \"retriever_type\": \"vector_index\", # \"vector_index\"\n",
    "    \"retriever_similarity_top_k\": 5,\n",
    "    \"query_mode\": \"hybrid\", # \"default\", \"hybrid\"\n",
    "    \"hybrid_search_alpha\": 0.0, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
    "    \"response_mode\": \"compact\",\n",
    "    \"use_reranker\": False,\n",
    "    \"rerank_top_k\": 3,\n",
    "\n",
    "    # Evaluation config\n",
    "    \"eval_llm_type\": \"openai\",\n",
    "    \"eval_llm_name\": \"gpt-3.5-turbo\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789ee5b-ceef-48c0-9e09-11ea2051233b",
   "metadata": {},
   "source": [
    "### Read secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbccb34-cf49-4661-bdb5-589e25e02d95",
   "metadata": {},
   "source": [
    "#### Weaviate Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2f405b-4c51-4174-ae8b-03029adec4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(Path.home() / \".weaviate.key\", \"r\")\n",
    "    f.close()\n",
    "except Exception as err:\n",
    "    print(f\"Could not read your Weaviate key. Please make sure this is available in plain text under your home directory in ~/.weaviate.key: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5959c-0bf5-414c-801d-c3a0ee8fbc24",
   "metadata": {},
   "source": [
    "#### Cohere API Key (required for re-ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a5832f-d1f5-402a-9f54-15e13a27bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(Path.home() / \".cohere.key\", \"r\")\n",
    "    os.environ[\"COHERE_API_KEY\"] = f.read().rstrip(\"\\n\")\n",
    "    f.close()\n",
    "except Exception as err:\n",
    "    print(f\"Could not read your Cohere API key. Please make sure this is available in plain text under your home directory in ~/.cohere.key: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c255d8-a01c-485d-b594-350bd57cc7ba",
   "metadata": {},
   "source": [
    "#### OpenAI API Key [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c136f7b0-6e18-4f99-aa3c-562620a3150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(Path.home() / \".openai.key\", \"r\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = f.read().rstrip(\"\\n\")\n",
    "    f.close()\n",
    "except Exception as err:\n",
    "    print(f\"Could not read your OpenAI API key. If you wish to run RAG evaluation, please make sure this is available in plain text under your home directory in ~/.openai.key: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63ff4f-fdcb-48db-8c88-87b602fb0f86",
   "metadata": {},
   "source": [
    "## STAGE 0 - Preliminary config checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e83dc8-9449-41d6-899d-0189bacaf437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_overlap': 0,\n",
      " 'chunk_size': 256,\n",
      " 'do_sample': False,\n",
      " 'embed_model_name': 'BAAI/bge-base-en-v1.5',\n",
      " 'embed_model_type': 'hf',\n",
      " 'eval_llm_name': 'gpt-3.5-turbo',\n",
      " 'eval_llm_type': 'openai',\n",
      " 'hybrid_search_alpha': 0.0,\n",
      " 'llm_name': 'Llama-2-7b-chat-hf',\n",
      " 'llm_type': 'local',\n",
      " 'max_new_tokens': 256,\n",
      " 'query_mode': 'hybrid',\n",
      " 'rerank_top_k': 3,\n",
      " 'response_mode': 'compact',\n",
      " 'retriever_similarity_top_k': 5,\n",
      " 'retriever_type': 'vector_index',\n",
      " 'temperature': 0.0,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0,\n",
      " 'use_reranker': False,\n",
      " 'vector_db_name': 'Pubmed_QA',\n",
      " 'vector_db_type': 'weaviate',\n",
      " 'weaviate_url': 'https://vector-rag-bootcamp-fqe3dp7f.weaviate.network'}\n"
     ]
    }
   ],
   "source": [
    "validate_rag_cfg(rag_cfg)\n",
    "pprint(rag_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672715b-12b7-424d-8410-859ff336a757",
   "metadata": {},
   "source": [
    "## STAGE 1 - Load dataset and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5fd2a-f974-4bb5-a678-7110aba1bc32",
   "metadata": {},
   "source": [
    "#### 1. Load PubMed QA dataset\n",
    "PubMedQA ([github](https://github.com/pubmedqa/pubmedqa)) is a biomedical question answering dataset. Each instance consists of a question, a context (extracted from PubMed abstracts), a long answer and a yes/no/maybe answer. We make use of the test split of [this](https://huggingface.co/datasets/bigbio/pubmed_qa) huggingface dataset for this notebook.\n",
    "\n",
    "**The context for each instance is stored as a text file** (referred to as documents), to align the task as a standard RAG use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd814ec-3084-4a5f-9fa9-8def165fe77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PubMed QA data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████| 389k/389k [00:00<00:00, 2.70MB/s]\n",
      "Downloading data: 100%|████████████████████| 55.1k/55.1k [00:00<00:00, 1.38MB/s]\n",
      "Downloading data: 100%|██████████████████████| 436k/436k [00:00<00:00, 4.56MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb196713027742b691297a541cc5f17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df84b3625ad453eb2cb1d73dd51c72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7406b2f8a0d48ab9a44956886bf67e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████| 492k/492k [00:00<00:00, 5.80MB/s]\n",
      "Downloading data: 100%|████████████████████| 66.4k/66.4k [00:00<00:00, 1.42MB/s]\n",
      "Downloading data: 100%|██████████████████████| 549k/549k [00:00<00:00, 4.75MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bc1e9346234175bb9a6526c77a0978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d750a0b90c8e43e5814e29b4d2d8229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06c149cb8004c17b91cfd26715fdf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data: 100%|███████████████████████| 500/500 [00:00<00:00, 1158.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data size: 500\n"
     ]
    }
   ],
   "source": [
    "print('Loading PubMed QA data ...')\n",
    "pubmed_data = PubMedQATaskDataset('bigbio/pubmed_qa')\n",
    "print(f\"Loaded data size: {len(pubmed_data)}\")\n",
    "pubmed_data.mock_knowledge_base(output_dir='./data', one_file_per_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139f09c-bf13-4d4a-9637-c7be7e165ad8",
   "metadata": {},
   "source": [
    "#### 2. Load documents\n",
    "All metadata is excluded by default. Set the *exclude_llm_metadata_keys* and *exclude_embed_metadata_keys* flags to *false* for including it. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html) and the *DocumentReader* class from *rag_utils.py* for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b8ff42-45ee-4ad2-bf72-ccca100e1286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents ...\n",
      "No. of documents loaded: 500\n"
     ]
    }
   ],
   "source": [
    "print('Loading documents ...')\n",
    "reader = DocumentReader(input_dir=\"./data/pubmed_doc\")\n",
    "docs = reader.load_data()\n",
    "print(f'No. of documents loaded: {len(docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db76ff-1c8c-4ada-8c3d-30b2ef7bca30",
   "metadata": {},
   "source": [
    "## STAGE 2 - Load node parser, embedding, LLM and set service context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e540f6-2522-4d69-89e6-3233f665c589",
   "metadata": {},
   "source": [
    "#### 1. Load node parser to split documents into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd11331-fe79-4100-bcd9-127838159e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading node parser ...\n"
     ]
    }
   ],
   "source": [
    "print('Loading node parser ...')\n",
    "node_parser = SentenceSplitter(chunk_size=rag_cfg['chunk_size'], chunk_overlap=rag_cfg['chunk_overlap'])\n",
    "# nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b8905-8a8e-4495-90f4-4e69811d9d19",
   "metadata": {},
   "source": [
    "#### 2. Load embedding model\n",
    "LlamaIndex supports embedding models from OpenAI, Cohere, HuggingFace, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#custom-embedding-model) for building a custom embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae31175-725c-46f9-ae43-5dc80f604649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hf embedding model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab24bd366de4a3db02041a00e488377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522a775c45c543dca18fdb67a4f42d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4a3bbc5db47509be5a6f3130ba05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb2a50b86014bd0945f465b50603592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3e77c29b73403a885a0d6f5c00d567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95fdcc475ae4325922ecc73367e43ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = RAGEmbedding(model_type=rag_cfg['embed_model_type'], model_name=rag_cfg['embed_model_name']).load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6446adf-983e-451c-b5dd-9a178e3b1af7",
   "metadata": {},
   "source": [
    "#### 3. Load LLM for generation\n",
    "LlamaIndex supports LLMs from OpenAI, Cohere, HuggingFace, AI21, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html#example-using-a-custom-llm-model-advanced) for loading a custom LLM model for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99126d7-48d6-4551-9c64-2044f7962ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local LLM model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711f411338e549a1ad6a642c43242f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = RAGLLM(rag_cfg['llm_type'], rag_cfg['llm_name']).load_model(**rag_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb554bd-9df1-444c-b75a-373e117eee06",
   "metadata": {},
   "source": [
    "#### 4. Use service context to set the node parser, embedding model, LLM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c62a809-0558-4e29-ae0d-451259663f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    ")\n",
    "# Set it globally to avoid passing it to every class, this sets it even for rag_utils.py\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dceb1e4-5ed6-47cb-987f-93f186387269",
   "metadata": {},
   "source": [
    "## STAGE 3 - Create index using the appropriate vector store\n",
    "All vector stores supported by LlamaIndex along with their available features are listed [here](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html).\n",
    "\n",
    "If you are using LangChain, the supported vector stores can be found [here](https://python.langchain.com/docs/modules/data_connection/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23824c74-c247-46ff-86fc-dcf1a8bd6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index ...\n"
     ]
    }
   ],
   "source": [
    "index = RAGIndex(db_type=rag_cfg['vector_db_type'], db_name=rag_cfg['vector_db_name'])\\\n",
    "    .create_index(docs, weaviate_url=rag_cfg[\"weaviate_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175a4a5-0a66-41b3-a848-850ce048bc6c",
   "metadata": {},
   "source": [
    "## STAGE 4 - Build query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d105b-87ca-4bac-a1b0-254f73297b0d",
   "metadata": {},
   "source": [
    "Now build a query engine using *retriever* and *response_synthesizer*. LlamaIndex also supports different types of [retrievers](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html) and [response modes](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#configuring-the-response-mode) for various use-cases.\n",
    "\n",
    "[Weaviate hybrid search](https://weaviate.io/blog/hybrid-search-explained) explains how dense and sparse search is combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce527d9f-9f18-444a-ab77-b110a5277a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_query_engine_args(rag_cfg, docs):\n",
    "    query_engine_args = {\n",
    "        \"similarity_top_k\": rag_cfg['retriever_similarity_top_k'], \n",
    "        \"response_mode\": rag_cfg['response_mode'],\n",
    "        \"use_reranker\": False,\n",
    "    }\n",
    "    \n",
    "    if (rag_cfg[\"retriever_type\"] == \"vector_index\") and (rag_cfg[\"vector_db_type\"] == \"weaviate\"):\n",
    "        query_engine_args.update({\n",
    "            \"query_mode\": rag_cfg[\"query_mode\"], \n",
    "            \"hybrid_search_alpha\": rag_cfg[\"hybrid_search_alpha\"]\n",
    "        })\n",
    "    elif rag_cfg[\"retriever_type\"] == \"bm25\":\n",
    "        nodes = service_context.node_parser.get_nodes_from_documents(docs)\n",
    "        tokenizer = service_context.embed_model._tokenizer\n",
    "        query_engine_args.update({\"nodes\": nodes, \"tokenizer\": tokenizer})\n",
    "        \n",
    "    if rag_cfg[\"use_reranker\"]:\n",
    "        query_engine_args.update({\"use_reranker\": True, \"rerank_top_k\": rag_cfg[\"rerank_top_k\"]})\n",
    "\n",
    "    return query_engine_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47bc0bd-3d83-4dce-b488-38806eed654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hybrid_search_alpha': 0.0,\n",
      " 'query_mode': 'hybrid',\n",
      " 'response_mode': 'compact',\n",
      " 'similarity_top_k': 5,\n",
      " 'use_reranker': False}\n"
     ]
    }
   ],
   "source": [
    "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
    "pprint(query_engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5502edfd-75d0-4b48-b75d-46e6adf9447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RAGQueryEngine(\n",
    "    retriever_type=rag_cfg['retriever_type'], vector_index=index, llm_model_name=rag_cfg['llm_name']).create(**query_engine_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6df972-2515-4968-9ea9-e2604f6ab82a",
   "metadata": {},
   "source": [
    "## STAGE 5 - Finally query the model!\n",
    "**Note:** We are using keyword based search or sparse search since *hybrid_search_alpha* is set to 0.0 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa9ea9-a3c9-4205-b050-d347a9f425dd",
   "metadata": {},
   "source": [
    "#### [TODO] Change seed to experiment with a different sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725791c9-371b-43d5-9d5f-9de5d01e04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd7abee9-59f7-481b-a644-45c107d10686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ['no'],\n",
      " 'context': 'Human immunodeficiency virus (HIV)-infected patients have '\n",
      "            'generally been excluded from transplantation. Recent advances in '\n",
      "            'the management and prognosis of these patients suggest that this '\n",
      "            'policy should be reevaluated. To explore the current views of '\n",
      "            'U.S. transplant centers toward transplanting asymptomatic '\n",
      "            'HIV-infected patients with end-stage renal disease, a written '\n",
      "            'survey was mailed to the directors of transplantation at all 248 '\n",
      "            'renal transplant centers in the United States. All 148 responding '\n",
      "            'centers said they require HIV testing of prospective kidney '\n",
      "            'recipients, and 84% of these centers would not transplant an '\n",
      "            'individual who refuses HIV testing. The vast majority of '\n",
      "            'responding centers would not transplant a kidney from a cadaveric '\n",
      "            '(88%) or a living donor (91%) into an asymptomatic HIV-infected '\n",
      "            'patient who is otherwise a good candidate for transplantation. '\n",
      "            'Among the few centers that would consider transplanting an '\n",
      "            'HIV-infected patient, not a single center had performed such a '\n",
      "            'transplant in the year prior to the survey. Most centers fear '\n",
      "            'that transplantation in the face of HIV infection would be '\n",
      "            'harmful to the individual, and some believe that it would be a '\n",
      "            'waste of precious organs.',\n",
      " 'id': '9603166',\n",
      " 'long_answer': 'The great majority of U.S. renal transplant centers will not '\n",
      "                'transplant kidneys to HIV-infected patients with end-stage '\n",
      "                'renal disease, even if their infection is asymptomatic. '\n",
      "                'However, advances in the management of HIV infection and a '\n",
      "                'review of relevant ethical issues suggest that this approach '\n",
      "                'should be reconsidered.',\n",
      " 'question': 'Should all human immunodeficiency virus-infected patients with '\n",
      "             'end-stage renal disease be excluded from transplantation?'}\n"
     ]
    }
   ],
   "source": [
    "sample_idx = random.randint(0, len(pubmed_data)-1)\n",
    "sample_elm = pubmed_data[sample_idx]\n",
    "pprint(sample_elm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8279db69-6525-401a-b39c-69f6b83cb0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
      "RESPONSE:  Based on the context information provided, I would say no to the query. The survey results suggest that while the majority of transplant centers require HIV testing of prospective kidney recipients, only a small percentage of centers would not transplant an HIV-infected patient who is otherwise a good candidate for transplantation. Additionally, some centers have performed transplants on HIV-infected patients in the past. These findings suggest that there is a possibility of transplanting HIV-infected patients with end-stage renal disease, and therefore, it is not necessary to exclude all HIV-infected patients with end-stage renal disease from transplantation.\n",
      "YES/NO: no\n",
      "GT ANSWER: ['no']\n",
      "GT LONG ANSWER: The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n"
     ]
    }
   ],
   "source": [
    "query = sample_elm['question']\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(f'QUERY: {query}')\n",
    "print(f'RESPONSE: {response}')\n",
    "print(f'YES/NO: {extract_yes_no(response.response)}')\n",
    "print(f'GT ANSWER: {sample_elm[\"answer\"]}')\n",
    "print(f'GT LONG ANSWER: {sample_elm[\"long_answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1808e94-337b-4e4b-9637-2b79746ddddc",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] [Ragas](https://docs.ragas.io/en/stable/index.html) evaluation\n",
    "Following are the commonly used metrics for evaluating a RAG workflow:\n",
    "* [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html): Measures the factual correctness of the generated answer based on the retrived context. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
    "* [Answer Relevance](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html): Measures how relevant the answer is to the given query. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
    "* [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html): Precision of the retriever as measured using the retrieved and the ground truth context. Value lies between 0 and 1.\n",
    "\n",
    "Additional metrics can be used based on the use-case:\n",
    "* [Context Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/context_relevancy.html)\n",
    "* [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html) (requires ground truth answer)\n",
    "* [Answer semantic similarity](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html) (requires ground truth answer)\n",
    "* [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html) (requires ground truth answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef3375ae-2cbf-4dfa-bc15-3fe32f4b6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = query_engine.retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bdb6372-9364-4bd3-9b81-bf403f0bc006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': [' Based on the context information provided, I would say no to the '\n",
      "            'query. The survey results suggest that while the majority of '\n",
      "            'transplant centers require HIV testing of prospective kidney '\n",
      "            'recipients, only a small percentage of centers would not '\n",
      "            'transplant an HIV-infected patient who is otherwise a good '\n",
      "            'candidate for transplantation. Additionally, some centers have '\n",
      "            'performed transplants on HIV-infected patients in the past. These '\n",
      "            'findings suggest that there is a possibility of transplanting '\n",
      "            'HIV-infected patients with end-stage renal disease, and '\n",
      "            'therefore, it is not necessary to exclude all HIV-infected '\n",
      "            'patients with end-stage renal disease from transplantation.'],\n",
      " 'contexts': [['Human immunodeficiency virus (HIV)-infected patients have '\n",
      "               'generally been excluded from transplantation. Recent advances '\n",
      "               'in the management and prognosis of these patients suggest that '\n",
      "               'this policy should be reevaluated. To explore the current '\n",
      "               'views of U.S. transplant centers toward transplanting '\n",
      "               'asymptomatic HIV-infected patients with end-stage renal '\n",
      "               'disease, a written survey was mailed to the directors of '\n",
      "               'transplantation at all 248 renal transplant centers in the '\n",
      "               'United States. All 148 responding centers said they require '\n",
      "               'HIV testing of prospective kidney recipients, and 84% of these '\n",
      "               'centers would not transplant an individual who refuses HIV '\n",
      "               'testing. The vast majority of responding centers would not '\n",
      "               'transplant a kidney from a cadaveric (88%) or a living donor '\n",
      "               '(91%) into an asymptomatic HIV-infected patient who is '\n",
      "               'otherwise a good candidate for transplantation. Among the few '\n",
      "               'centers that would consider transplanting an HIV-infected '\n",
      "               'patient, not a single center had performed such a transplant '\n",
      "               'in the year prior to the survey. Most centers fear that '\n",
      "               'transplantation in the face of HIV infection would be harmful '\n",
      "               'to the individual, and some believe that it would be a waste '\n",
      "               'of precious organs.',\n",
      "               'At least one marker of hepatitis G virus infection (hepatitis '\n",
      "               'G virus-RNA and/or anti-hepatitis G virus, mostly mutually '\n",
      "               'exclusive) was present in 6 out of 23 patients with '\n",
      "               'cryptogenic hepatitis and 16 out of 40 with hepatitis C virus '\n",
      "               'liver disease (26. 1% vs 40% p=ns). T virus-DNA was present in '\n",
      "               'serum in 3 subjects, 1 with cryptogenic and 2 with hepatitis C '\n",
      "               'virus-related chronic liver disease. Demographic and clinical '\n",
      "               'features, including stage and grade of liver histology, were '\n",
      "               'comparable between hepatitis G virus-infected and uninfected '\n",
      "               'subjects. Severe liver damage [chronic hepatitis with fibrosis '\n",
      "               'or cirrhosis) were significantly more frequent in subjects '\n",
      "               'with hepatitis C virus liver disease.',\n",
      "               'Hepatitis G virus can cause chronic infection in man but the '\n",
      "               'role of this agent in chronic liver disease is poorly '\n",
      "               'understood. Little is known about the relation of another '\n",
      "               'newly discovered agent, the TT virus, with chronic liver '\n",
      "               'disease.AIM: To investigate the rate of infection with '\n",
      "               'hepatitis G virus and TT virus in patients with cryptogenic '\n",
      "               'chronic liver disease. A total of 23 subjects with chronically '\n",
      "               'raised alanine transaminase and a liver biopsy in whom all '\n",
      "               'known causes of liver disease had been excluded, and 40 '\n",
      "               'subjects with hepatitis C virus-related chronic liver disease. '\n",
      "               'Evaluation of anti-hepatitis G virus by enzyme immunoassay. '\n",
      "               'Hepatitis G virus-RNA by polymerase chain reaction with '\n",
      "               \"primers from the 5' NC and NS5a regions. TT virus-DNA by \"\n",
      "               'nested polymerase chain reaction with primers from the ORF1 '\n",
      "               'region. Results. Hepatitis G virus-RNA was detected in 4 out '\n",
      "               'of 23 patients with cryptogenic chronic hepatitis and in 6 out '\n",
      "               'of 40 with hepatitis C virus chronic hepatitis (17.4% vs 15% '\n",
      "               'p=ns).',\n",
      "               'Hepatorenal syndrome (HRS) is the functional renal failure '\n",
      "               'associated with advanced cirrhosis and has also been described '\n",
      "               'in fulminant hepatic failure. Without liver transplantation '\n",
      "               'its prognosis is dismal. Our study included patients with type '\n",
      "               '1 HRS associated with cirrhosis, who were not liver transplant '\n",
      "               'candidates.AIM: To identify variables associated with improved '\n",
      "               'survival. Sixty-eight patients fulfilled the revised Ascites '\n",
      "               'Club Criteria for type 1 HRS. None of them was suitable for '\n",
      "               'liver transplantation. All the patients were treated with '\n",
      "               'combinations of: albumin, midodrine and octreotide, pressors, '\n",
      "               'and hemodialysis. Median survival was 13 days for the whole '\n",
      "               'group. Survival varied with the end-stage liver disease (ESLD) '\n",
      "               'etiology: autoimmune, 49 days, cardiac cirrhosis, 22 days, '\n",
      "               'idiopathic, 15.5 days, viral, 15 days, hepatitis C and '\n",
      "               'alcohol, 14.5 days, alcohol 8 days, and neoplasia 4 days (p = '\n",
      "               '0.048). Survival of HRS associated with alcoholic liver '\n",
      "               'disease versus other etiologies was not statistically '\n",
      "               'significant (p = 0.1).',\n",
      "               'A higher prevalence of cardiovascular risk factors (CRFs) in '\n",
      "               'HIV-infected patients, together with chronic infection and '\n",
      "               'treatments, has resulted in an increased risk of silent '\n",
      "               'myocardial ischaemia (SMI). The objective of this study was to '\n",
      "               'evaluate whether myocardial SPECT should be used for screening '\n",
      "               'HIV-infected patients with no clinical symptoms of coronary '\n",
      "               'artery disease. The prevalence of SMI detected by myocardial '\n",
      "               'SPECT was determined in 94 HIV-infected patients with a normal '\n",
      "               'clinical cardiovascular examination in relation to '\n",
      "               'anthropomorphic parameters, CRFs, inflammatory and HIV '\n",
      "               'infection status, and treatment. Coronary artery disease was '\n",
      "               'detected in nine patients (eight with ischaemia, one with '\n",
      "               'myocardial infarction), corresponding to 9.6 % positivity. All '\n",
      "               'but two of the scintigraphic diagnoses of ischaemia were '\n",
      "               'confirmed by coronarography. Univariate analysis revealed that '\n",
      "               'the overall number of CRFs and the combination of gender and '\n",
      "               'age were associated with a diagnosis of SMI (p<0.05). '\n",
      "               'According to multivariate analysis, the only independent '\n",
      "               'parameter significantly associated with the scintigraphic '\n",
      "               'diagnosis of SMI was the combination of gender and age (p = '\n",
      "               '0.01).']],\n",
      " 'ground_truths': [['The great majority of U.S. renal transplant centers will '\n",
      "                    'not transplant kidneys to HIV-infected patients with '\n",
      "                    'end-stage renal disease, even if their infection is '\n",
      "                    'asymptomatic. However, advances in the management of HIV '\n",
      "                    'infection and a review of relevant ethical issues suggest '\n",
      "                    'that this approach should be reconsidered.']],\n",
      " 'question': ['Should all human immunodeficiency virus-infected patients with '\n",
      "              'end-stage renal disease be excluded from transplantation?']}\n"
     ]
    }
   ],
   "source": [
    "eval_data = {\n",
    "    \"question\": [query],\n",
    "    \"answer\": [response.response],\n",
    "    \"contexts\": [[node.text for node in retrieved_nodes]],\n",
    "    \"ground_truths\": [[sample_elm['long_answer']]],\n",
    "    }\n",
    "pprint(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed0582ac-b840-457d-86eb-4aa9d8fc43de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04384eb92a97449690e875598d18089b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 75, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 63, in _aresults\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py\", line 180, in _ascore\n",
      "    answer_result = await self.llm.generate(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 571, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 434, in generate\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 424, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 608, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 438, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_obj \u001b[38;5;241m=\u001b[39m RagasEval(\n\u001b[1;32m      2\u001b[0m             metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      3\u001b[0m             eval_llm_type\u001b[38;5;241m=\u001b[39mrag_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_llm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m], eval_llm_name\u001b[38;5;241m=\u001b[39mrag_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_llm_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m             )\n\u001b[0;32m----> 5\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43meval_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_result)\n",
      "File \u001b[0;32m/fs01/home/ws_dnourian/daniel_rag_march19/pubmed_qa/utils/rag_utils.py:297\u001b[0m, in \u001b[0;36mRagasEval.evaluate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m    296\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_data(data)\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mragas_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, max_workers, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    230\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[1;32m    235\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[1;32m    236\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    237\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[1;32m    238\u001b[0m     )\n",
      "File \u001b[0;32m/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/evaluation.py:214\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, max_workers, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "eval_obj = RagasEval(\n",
    "            metrics=[\"faithfulness\", \"relevancy\", \"precision\"], \n",
    "            eval_llm_type=rag_cfg[\"eval_llm_type\"], eval_llm_name=rag_cfg[\"eval_llm_name\"]\n",
    "            )\n",
    "eval_result = eval_obj.evaluate(eval_data)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5bcc7-900b-4939-aa71-65385b5c765c",
   "metadata": {},
   "source": [
    "### 5.1 - Dense Search\n",
    "Set *hybrid_search_alpha* to 1.0 for dense vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "131eaa05-ae98-481c-9967-cc6c74b9d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg[\"hybrid_search_alpha\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fba7d70-0bd7-42b7-ba52-f1924e96b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hybrid_search_alpha': 1.0,\n",
      " 'query_mode': 'hybrid',\n",
      " 'response_mode': 'compact',\n",
      " 'similarity_top_k': 5,\n",
      " 'use_reranker': False}\n",
      "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
      "RESPONSE:  Based on the context information provided, I would say no to the query. The survey results showed that while the majority of transplant centers require HIV testing of prospective kidney recipients and would not transplant an individual who refuses HIV testing, there are some centers that would consider transplanting an HIV-infected patient. Additionally, the study on elderly kidneys found that there were no differences in survival rates between elderly and younger recipients. These findings suggest that HIV-infected patients with end-stage renal disease should not be automatically excluded from transplantation.\n",
      "YES/NO: no\n",
      "GT ANSWER: ['no']\n",
      "GT LONG ANSWER: The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n"
     ]
    }
   ],
   "source": [
    "# Recreate query engine\n",
    "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
    "pprint(query_engine_args)\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever_type=rag_cfg['retriever_type'], vector_index=index, llm_model_name=rag_cfg['llm_name']).create(**query_engine_args)\n",
    "\n",
    "# Get response\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print response\n",
    "print(f'QUERY: {query}')\n",
    "print(f'RESPONSE: {response}')\n",
    "print(f'YES/NO: {extract_yes_no(response.response)}')\n",
    "print(f'GT ANSWER: {sample_elm[\"answer\"]}')\n",
    "print(f'GT LONG ANSWER: {sample_elm[\"long_answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b4da5-2391-49ca-bbe6-ed7f2433faa5",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Ragas evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78b240d3-2b09-4c28-8c20-6d9be47dc963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ad032e4804cd8bf28835b344fd2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 75, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 63, in _aresults\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/metrics/_answer_relevance.py\", line 136, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 571, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 434, in generate\n",
      "    raise e\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 424, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 608, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 438, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m retrieved_nodes \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39mretrieve(query)\n\u001b[1;32m      3\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: [query],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response\u001b[38;5;241m.\u001b[39mresponse],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[node\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m retrieved_nodes]],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truths\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[sample_elm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]]],\n\u001b[1;32m      8\u001b[0m     }\n\u001b[0;32m---> 10\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43meval_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_result)\n",
      "File \u001b[0;32m/fs01/home/ws_dnourian/daniel_rag_march19/pubmed_qa/utils/rag_utils.py:297\u001b[0m, in \u001b[0;36mRagasEval.evaluate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m    296\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_data(data)\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mragas_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, max_workers, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    230\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[1;32m    235\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[1;32m    236\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    237\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[1;32m    238\u001b[0m     )\n",
      "File \u001b[0;32m/fs01/projects/aieng/public/rag_bootcamp/envs/rag_pubmed_qa/lib/python3.10/site-packages/ragas/evaluation.py:214\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, max_workers, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
    "\n",
    "eval_data = {\n",
    "    \"question\": [query],\n",
    "    \"answer\": [response.response],\n",
    "    \"contexts\": [[node.text for node in retrieved_nodes]],\n",
    "    \"ground_truths\": [[sample_elm['long_answer']]],\n",
    "    }\n",
    "\n",
    "eval_result = eval_obj.evaluate(eval_data)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69911c71-1639-4ed8-86cb-85abbbc15467",
   "metadata": {},
   "source": [
    "### 5.2 - Hybrid Search\n",
    "Set *hybrid_search_alpha* to 0.5 for hybrid search with equal weightage for dense and sparse (keyword-based) search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a526b44-5473-4850-9209-7f9b9446f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg[\"hybrid_search_alpha\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf513126-01fe-4f6f-b18a-b6c5224f07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate query engine\n",
    "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
    "pprint(query_engine_args)\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever_type=rag_cfg['retriever_type'], vector_index=index, llm_model_name=rag_cfg['llm_name']).create(**query_engine_args)\n",
    "\n",
    "# Get response\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print response\n",
    "print(f'QUERY: {query}')\n",
    "print(f'RESPONSE: {response}')\n",
    "print(f'YES/NO: {extract_yes_no(response.response)}')\n",
    "print(f'GT ANSWER: {sample_elm[\"answer\"]}')\n",
    "print(f'GT LONG ANSWER: {sample_elm[\"long_answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7abc-1fbd-4d66-a724-dddba4733721",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Ragas evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03197082-eb44-46d7-b982-816f8811560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
    "\n",
    "eval_data = {\n",
    "    \"question\": [query],\n",
    "    \"answer\": [response.response],\n",
    "    \"contexts\": [[node.text for node in retrieved_nodes]],\n",
    "    \"ground_truths\": [[sample_elm['long_answer']]],\n",
    "    }\n",
    "\n",
    "eval_result = eval_obj.evaluate(eval_data)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca6e00-2580-4d98-a832-3fd0f8094e1a",
   "metadata": {},
   "source": [
    "### 5.3 - Using Re-ranker\n",
    "Set *use_reranker* to *True* to re-rank the context after retrieving it from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad8e06-e9a1-4d3c-b6e5-fa8bc6b1b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg[\"use_reranker\"] = True\n",
    "rag_cfg[\"hybrid_search_alpha\"] = 1.0 # Using dense search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c714b8f-45db-4ab6-9e28-7afa8c8a31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate query engine\n",
    "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
    "pprint(query_engine_args)\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever_type=rag_cfg['retriever_type'], vector_index=index, llm_model_name=rag_cfg['llm_name']).create(**query_engine_args)\n",
    "\n",
    "# Get response\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print response\n",
    "print(f'QUERY: {query}')\n",
    "print(f'RESPONSE: {response}')\n",
    "print(f'YES/NO: {extract_yes_no(response.response)}')\n",
    "print(f'GT ANSWER: {sample_elm[\"answer\"]}')\n",
    "print(f'GT LONG ANSWER: {sample_elm[\"long_answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f4714-ee80-46ba-bf14-ec4762f70a08",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Ragas evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43ba3a-6824-4246-8df9-3e9ce7efa5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
    "\n",
    "eval_data = {\n",
    "    \"question\": [query],\n",
    "    \"answer\": [response.response],\n",
    "    \"contexts\": [[node.text for node in retrieved_nodes]],\n",
    "    \"ground_truths\": [[sample_elm['long_answer']]],\n",
    "    }\n",
    "\n",
    "eval_result = eval_obj.evaluate(eval_data)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36ce4b-a974-48af-8b24-d9b9be9ace22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pubmed_qa",
   "language": "python",
   "name": "rag_pubmed_qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
