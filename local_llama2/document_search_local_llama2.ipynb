{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af413b8",
   "metadata": {},
   "source": [
    "# Local Llama2 Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a3fc9",
   "metadata": {},
   "source": [
    "This example demonstrates a RAG workflow that uses locally-stored documents to augment the result of an LLM query. Notably, this entire workflow is implemented with locally-hosted models and open source utilities. This will be especially useful for RAG workflows that involve sensitive data (medical, legal, financial) which you might not want to send to OpenAI or Cohere for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8e317",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4e1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from llama_index.core import ServiceContext, set_global_service_context, set_global_handler, SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "from utils.hosting_utils import RAGLLM\n",
    "from utils.rag_utils import RAGEmbedding\n",
    "from utils.storage_utils import RAGIndex\n",
    "\n",
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43490a5",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa721ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc6ec5-f5a9-4881-ba54-ed7f3ab203a9",
   "metadata": {},
   "source": [
    "In order to load the Llama2 model, you need to have a copy of the model weights saved in a folder called **/model-weights/Llama-2-7b-chat-hf**. Make sure this folder exists and we have at least a copy of the tozenizer there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f66ec52-5c41-42ad-8519-f9eadf0a7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_tokenizer = False\n",
    "model_weights_path = \"/model-weights/Llama-2-7b-chat-hf\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    print(f\"ERROR: The {model_weights_path} folder must exist, containing the Llama2 model weights\")\n",
    "for filename in os.listdir(model_weights_path):\n",
    "    contains_tokenizer = True if \"tokenizer.model\" in filename else contains_tokenizer\n",
    "if not contains_tokenizer:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least the tokenizer.model file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab705cd1",
   "metadata": {},
   "source": [
    "## Bring up a locally-hosted Llama2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be61323",
   "metadata": {},
   "source": [
    "Define the RAG configuration. You should try modifying the configuration values below to see how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117d1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg = {\n",
    "    # Node parser config\n",
    "    \"chunk_size\": 256,\n",
    "    \"chunk_overlap\": 0,\n",
    "\n",
    "    # Embedding model config\n",
    "    \"embed_model_type\": \"hf\",\n",
    "    \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
    "\n",
    "    # LLM config\n",
    "    \"llm_type\": \"local\",\n",
    "    \"llm_name\": \"Llama-2-13b-chat-hf\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 50,\n",
    "    \"do_sample\": False,\n",
    "\n",
    "    # Vector DB config\n",
    "    \"vector_db_type\": \"chromadb\",\n",
    "    \"vector_db_name\": \"local_llama2\",\n",
    "\n",
    "    # Retriever and query config\n",
    "    \"retriever_type\": \"vector_index\", # \"vector_index\", \"bm25\"\n",
    "    \"retriever_similarity_top_k\": 3,\n",
    "    \"query_mode\": \"hybrid\", # \"default\", \"hybrid\"\n",
    "    \"hybrid_search_alpha\": 0.5, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
    "    \"response_mode\": \"compact\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3875ff",
   "metadata": {},
   "source": [
    "Load a local Llama2 LLM for generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907255b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local LLM model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e01973a08fc446eb23e5480036d99cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Can we check if the model is already loaded?\n",
    "llm = RAGLLM(rag_cfg['llm_type'], rag_cfg['llm_name']).load_model(**rag_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726b580",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd204f8b",
   "metadata": {},
   "source": [
    "Let's start by asking Llama2 a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's basic knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is very domain-specific that it won't know the answer to. A good example would an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "\"*How many Vector scholarships in AI were awarded in 2022?*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecc11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01a2f2",
   "metadata": {},
   "source": [
    "Now, send the generation request.\n",
    "\n",
    "**Important note**: In the demo output below, you'll notice a big red CUDA error. Unfortunately I am running this example on a personal workstation using an NVIDIA TitanXP GPU, which has a terribly insufficient amount of GPU memory. Typically you'll want to run this on at least an NVIDIA A40 (48 GB GPU memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4805e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aea26e",
   "metadata": {},
   "source": [
    "Without additional information, our local Llama2 model is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022**. Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the source-materials folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36ed1b",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e0b80",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from source-materials, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(pdf_folder_path).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")\n",
    "print(f\"Example first source material:\\n {documents[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35954672",
   "metadata": {},
   "source": [
    "Load node parser to split documents into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae02ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=rag_cfg['chunk_size'], chunk_overlap=rag_cfg['chunk_overlap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc440fcf",
   "metadata": {},
   "source": [
    "Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = RAGEmbedding(\n",
    "    model_type=rag_cfg['embed_model_type'], \n",
    "    model_name=rag_cfg['embed_model_name']\n",
    ").load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a46951",
   "metadata": {},
   "source": [
    "Use service context to set the node parser, embedding model, LLM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb555661",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Set it globally to avoid passing it to every class, this sets it even for rag_utils.py\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dc715",
   "metadata": {},
   "source": [
    "Create index using the appropriate vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = RAGIndex(db_type=rag_cfg['vector_db_type'], db_name=rag_cfg['vector_db_name']).create_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ce508",
   "metadata": {},
   "source": [
    "## Now perform the RAG-augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d66a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69145821-c4d3-464e-bbbb-ef4749479f90",
   "metadata": {},
   "source": [
    "Make sure to use a GPU with enough onboard memory to handle the generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00738a50-11bc-4ed2-87a2-82d5f50f3148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
